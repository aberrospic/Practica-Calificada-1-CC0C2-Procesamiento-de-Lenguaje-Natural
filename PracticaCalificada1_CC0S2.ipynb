{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "e35a97d0",
      "metadata": {
        "id": "e35a97d0"
      },
      "source": [
        "## Primera práctica calificada\n",
        "\n",
        "**Nombre y Apellidos: Abraham Berrospi Casano**\n",
        "\n",
        "**Código: 20171567J**\n",
        "\n",
        "### Reglas para la Prueba Calificada\n",
        "\n",
        "- Queda terminantemente prohibido el uso de herramientas como ChatGPT, WhatsApp, o cualquier herramienta similar durante la realización de esta prueba. El uso de estas herramientas, por cualquier motivo, resultará en la anulación inmediata de la evaluación.\n",
        "\n",
        "- Las respuestas deben presentarse con una explicación detallada, utilizando términos técnicos apropiados. La mera descripción sin el uso de terminología técnica, especialmente términos discutidos en clase, se considerará insuficiente y podrá resultar en que la respuesta sea marcada como incorrecta.\n",
        "\n",
        "\n",
        "- Cada estudiante debe presentar su propio trabajo. Los códigos iguales o muy parecidos entre sí serán considerados como una violación a la integridad académica, implicando una copia, y serán sancionados de acuerdo con las políticas de la universidad.\n",
        "\n",
        "- Todos los estudiantes deben subir sus repositorios de código a la plataforma del curso, según las instrucciones proporcionadas. La fecha y hora de la última actualización del repositorio serán consideradas como la hora de entrega.\n",
        "\n",
        "- La claridad, orden, y presentación general de las evaluaciones serán tomadas en cuenta en la calificación final. Se espera un nivel de profesionalismo en la documentación y presentación del código y las respuestas escritas.\n",
        "\n",
        "\n",
        "#### Instrucciones de entrega para la prueba calificada\n",
        "\n",
        "- Presenta la dirección de tu repositorio personal donde se encuentre este cuaderno con tus respuestas desarrolladas.\n",
        "- Todo cambio fuera de la hora y fecha del examen realizado dentro del repositorio no se tomará en cuenta y se procederá a anular la evaluación."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c6884a0",
      "metadata": {
        "id": "3c6884a0"
      },
      "source": [
        "1 . Dividir un bloque de texto en subunidades significativas es una parte esencial del procesamiento de texto. El texto se puede dividir en caracteres o palabras individuales o en algún punto intermedio. A continuación se muestra un enfoque muy básico que divide el texto utilizando espacios en blanco. Ya existe un defecto, ya que la última palabra `dog` tiene puntuación.\n",
        "\n",
        "```\n",
        "'The quick brown fox jumps over the lazy dog.'.split(' ')\n",
        "```\n",
        "\n",
        "Con los modelos Transformer, realizamos tokenizaciones de subpalabras y dividimos el texto mediante un tokenizador prediseñado. Esto ha sido entrenado en una gran cantidad de texto donde ha aprendido cuáles son palabras comunes y cuáles son menos comunes y podrían dividirse en partes (que a menudo parecen sílabas).\n",
        "\n",
        "Primero, carguemos uno para un modelo de Transformer común `distilgpt2`. Podemos cargarlo con el siguiente código. El modelo distilgpt2 es un modelo más pequeño basado en gpt2 que vimos en clase, que es un predecesor del modelo de lenguaje que sustenta ChatGPT.\n",
        "\n",
        "```\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilgpt2')\n",
        "```\n",
        "\n",
        "El tokenizador tiene una función `tokenizer.tokenize`  que divide el texto, así como funciones como `tokenizer.vocab`, `tokenizer.encode`, `tokenizer.decode`. El tokenizador tiene muchos parámetros para brindar control adicional. Por ejemplo, a veces es necesario truncar cadenas muy largas (ya que existe un límite en la longitud de entrada a los modelos Transformer). Puedes utilizar la función `tokenizer.encode` para tokenizar una oración como \"Kelvingrove is a beautiful park in Glasgow.\" que puedes recortar a solo 5 tokens usando `truncation=True` y `max_length=5`.\n",
        "\n",
        "\n",
        "Revisa: https://huggingface.co/transformers/v3.0.2/model_doc/auto.html\n",
        "\n",
        "\n",
        "**Preguntas:**\n",
        "\n",
        "* Encuentra una palabra en inglés que sea tokenizada en 3, 4, 5 o incluso 6 subtokens con el tokenizador distilgpt2 (1 punto)\n",
        "* Escribe una función en Python  para tokenizar un texto y mostrar los tokens y sus IDs, también escribe una función que utilice el tokenizador `datificate/gpt2-small-spanish` para identificar palabras que se descomponen en 3, 4, 5 o 6 subtokens, lo cual es interesante para entender la granularidad del tokenizador (2 puntos).\n",
        "\n",
        "* Al tokenizar, utilizaremos el tokenizador con el parámetro `return_tensors='pt'`. Esto coloca los datos en el formato de un tensor PyTorch, que se utiliza como entrada para un modelo Transformer. PyTorch es una biblioteca comúnmente utilizada para el aprendizaje profundo y HuggingFace se basa en ella. No usaremos PyTorch directamente. Tokeniza: `\"A horse! a horse! my kingdom for a\"` (1 punto)\n",
        "\n",
        "* Implementar un script en Python que use AutoModelForCausalLM para cargar un modelo de lenguaje causal. El ejercicio consistirá en tokenizar un texto, pasarlo a un modelo Transformer, y luego analizar las probabilidades de los tokens generados para identificar el más probable. Revisa: https://huggingface.co/docs/transformers/tasks/language_modeling (3 puntos)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6409c639",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6409c639",
        "outputId": "e92922a5-8834-413a-b42c-514583310916"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "a9b98310",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9b98310",
        "outputId": "5096cd9f-a98f-4028-f566-8256b5b9f039"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La palabra \"fantastic\" se tokeniza en 3 subtokens: ['f', 'ant', 'astic']\n",
            "La palabra \"Consanguineous\" se tokeniza en 4 subtokens: ['Cons', 'angu', 'ine', 'ous']\n"
          ]
        }
      ],
      "source": [
        "## Tus respuestas\n",
        "\n",
        "'''\n",
        "Encuentra una palabra en inglés que sea tokenizada en 3, 4, 5 o incluso 6 subtokens con el tokenizador distilgpt2\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
        "\n",
        "word1 = 'fantastic'\n",
        "word2 = 'Consanguineous'\n",
        "\n",
        "tokens1 = tokenizer.tokenize(word1)\n",
        "tokens2 = tokenizer.tokenize(word2)\n",
        "\n",
        "print(f'La palabra \"{word1}\" se tokeniza en {len(tokens1)} subtokens: {tokens1}')\n",
        "print(f'La palabra \"{word2}\" se tokeniza en {len(tokens2)} subtokens: {tokens2}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Escribe una función en Python para tokenizar un texto y mostrar los tokens y sus IDs, ...\n",
        "'''\n",
        "\n",
        "import spacy\n",
        "import en_core_web_sm\n",
        "\n",
        "# Cargar el modelo de idioma de spaCy\n",
        "nlp = en_core_web_sm.load()\n",
        "\n",
        "texto = \"London is the capital and most populous city of England and the United Kingdom.\"\n",
        "\n",
        "doc = nlp(texto)\n",
        "# Imprimir los tokens con sus IDs\n",
        "for i, token in enumerate(doc):\n",
        "  print(f'Token {i+1}: {token.text}, ID: {i}')\n",
        "\n",
        "print()\n",
        "\n",
        "'''\n",
        "... también escribe una función que utilice el tokenizador datificate/gpt2-small-spanish para\n",
        "identificar palabras que se descomponen en 3, 4, 5 o 6 subtokens, lo cual es interesante\n",
        "para entender la granularidad del tokenizador\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"datificate/gpt2-small-spanish\")\n",
        "\n",
        "def PalabrasConMasDeTresSubtokensSpanish(word):\n",
        "  tokens = tokenizer.tokenize(word)\n",
        "  if (len(tokens) > 2) :\n",
        "    print(f'La palabra \"{word}\" se descompone en {len(tokens)} subtokens: {tokens}')\n",
        "  else :\n",
        "    print(f'La palabra \"{word}\" no se descompone en más de 3 subtokens')\n",
        "\n",
        "\n",
        "word1 = 'Antidemocrático'\n",
        "word2 = 'comida'\n",
        "\n",
        "PalabrasConMasDeTresSubtokensSpanish(word1)\n",
        "PalabrasConMasDeTresSubtokensSpanish(word2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fi3vDVXzVtQa",
        "outputId": "0b0c4f0e-63ae-4d84-dd70-bba7deb181b0"
      },
      "id": "fi3vDVXzVtQa",
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token 1: London, ID: 0\n",
            "Token 2: is, ID: 1\n",
            "Token 3: the, ID: 2\n",
            "Token 4: capital, ID: 3\n",
            "Token 5: and, ID: 4\n",
            "Token 6: most, ID: 5\n",
            "Token 7: populous, ID: 6\n",
            "Token 8: city, ID: 7\n",
            "Token 9: of, ID: 8\n",
            "Token 10: England, ID: 9\n",
            "Token 11: and, ID: 10\n",
            "Token 12: the, ID: 11\n",
            "Token 13: United, ID: 12\n",
            "Token 14: Kingdom, ID: 13\n",
            "Token 15: ., ID: 14\n",
            "\n",
            "La palabra \"Antidemocrático\" se descompone en 3 subtokens: ['Anti', 'demo', 'crÃ¡tico']\n",
            "La palabra \"comida\" no se descompone en más de 3 subtokens\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Al tokenizar, utilizaremos el tokenizador con el parámetro return_tensors='pt'.\n",
        "Esto coloca los datos en el formato de un tensor PyTorch, que se utiliza como entrada para un modelo Transformer.\n",
        "PyTorch es una biblioteca comúnmente utilizada para el aprendizaje profundo y HuggingFace se basa en ella.\n",
        "No usaremos PyTorch directamente. Tokeniza: \"A horse! a horse! my kingdom for a\"\n",
        "'''\n",
        "\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n",
        "\n",
        "text = \"A horse! a horse! my kingdom for a\"\n",
        "\n",
        "encoding = tokenizer(text, return_tensors=None)"
      ],
      "metadata": {
        "id": "spuvvkyJV2MQ"
      },
      "id": "spuvvkyJV2MQ",
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenizar el texto sin PyTorch\n",
        "tokens = tokenizer.tokenize(texto)\n",
        "\n",
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SihXqCoYg6tn",
        "outputId": "5cba3995-86a8-4c1f-c3a8-43b5e224eb6f"
      },
      "id": "SihXqCoYg6tn",
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "A horse! a horse! my kingdom for a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Implementar un script en Python que use AutoModelForCausalLM para cargar un modelo de lenguaje causal.\n",
        "El ejercicio consistirá en tokenizar un texto, pasarlo a un modelo Transformer, y luego analizar las\n",
        "probabilidades de los tokens generados para identificar el más probable.\n",
        "'''\n",
        "\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, top_k_top_p_filtering\n",
        "import torch\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# Cargar tokenizador y modelo distilgpt2\n",
        "tokenizer = AutoTokenizer.from_pretrained('distilbert/distilgpt2')\n",
        "model = AutoModelForCausalLM.from_pretrained('distilbert/distilgpt2')\n",
        "\n",
        "# Tokenizar frase de entrada\n",
        "phrase = f'I sleep in a bed that is poorly '\n",
        "inputs = tokenizer.encode(phrase, return_tensors='pt')\n",
        "\n",
        "# Obtener logits de la última capa\n",
        "last_layer_logits = model(inputs).logits[:, -1, :]\n",
        "\n",
        "# Mantener los 30 logits principales al máximo; detenerse si la probabilidad acumulada >= 1,0.\n",
        "top_logits = top_k_top_p_filtering(last_layer_logits, top_k=30, top_p=1.0)\n",
        "\n",
        "# Softmax los logits en probabilidades\n",
        "probabilities = F.softmax(top_logits, dim=-1)\n",
        "\n",
        "# Generar el siguiente token\n",
        "generated_next_token = torch.multinomial(probabilities, num_samples=1)\n",
        "generated = torch.cat([inputs, generated_next_token], dim=-1)\n",
        "\n",
        "# Obtener resultado\n",
        "result_string = tokenizer.decode(generated.tolist()[0])\n",
        "\n",
        "# Imprimir cadena\n",
        "print(result_string)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2HFLNweV29y",
        "outputId": "77babe29-86e6-41b5-9b69-b1c07b278054"
      },
      "id": "d2HFLNweV29y",
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I sleep in a bed that is poorly iced\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e017711b",
      "metadata": {
        "id": "e017711b"
      },
      "source": [
        "2 . En estos ejercicios se trata de profundizar en los modelos CBOW y Skim-Gram visto en clase:\n",
        "\n",
        "* Implementa un algoritmo para descubrir todos los 2-skip-2-gramas de una oración dada (2 puntos)\n",
        "* Expresa la función de pérdida únicamente como función de las entradas y los pesos, después de eliminar las variables de la capa oculta. (1 punto)\n",
        "\n",
        "* Calcula los gradientes de la función de pérdida con respecto a los pesos en las capas de entrada y salida (2 puntos)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "3469828f",
      "metadata": {
        "id": "3469828f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1028a8da-7e4e-4ce7-ebb5-fce20d2c0ef4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "El gato\n",
            "El come\n",
            "El pescado\n",
            "gato come\n",
            "gato pescado\n",
            "come pescado\n"
          ]
        }
      ],
      "source": [
        "def generate_skip_grams(sequence, n, k):\n",
        "    skip_grams = []\n",
        "\n",
        "    def recurse_gram(base, start, depth):\n",
        "        if depth == n:\n",
        "            skip_grams.append(base)\n",
        "            return\n",
        "        for i in range(start, min(len(sequence), start + k + 1)):\n",
        "            recurse_gram(base + [sequence[i]], i + 1, depth + 1)\n",
        "\n",
        "    for i in range(len(sequence)):\n",
        "        recurse_gram([sequence[i]], i + 1, 1)\n",
        "\n",
        "    return skip_grams\n",
        "\n",
        "# Ejemplo de uso\n",
        "sequence = [\"El\", \"gato\", \"come\", \"pescado\"]\n",
        "n = 2  # Queremos 2-gramas\n",
        "k = 2  # Permitimos dos skips\n",
        "skip_grams = generate_skip_grams(sequence, n, k)\n",
        "for sg in skip_grams:\n",
        "    print(\" \".join(sg))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Expresa la función de pérdida únicamente como función de las entradas y los pesos,\n",
        "después de eliminar las variables de la capa oculta.\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "pBEqTrp6gI9N"
      },
      "id": "pBEqTrp6gI9N",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''\n",
        "Calcula los gradientes de la función de pérdida con respecto a los pesos en las capas de entrada y salida\n",
        "'''\n",
        "\n"
      ],
      "metadata": {
        "id": "fXc-xtEEgJ__"
      },
      "id": "fXc-xtEEgJ__",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "37b0c6e6",
      "metadata": {
        "id": "37b0c6e6"
      },
      "source": [
        "3 . De la actividad de modelos de lenguaje realizada en clase, las técnicas de suavizado, como el suavizado de Laplace, a menudo se agregan a los modelos de lenguaje de n-gramas para manejar probabilidades de 0. ¿Cómo podrías modificar tu código para incluirlo?.\n",
        "Puedes tambien experimentar con el suavizado de Good-Turing que ajusta las cuentas de los n-gramas basándose en la frecuencia de frecuencias de n-gramas. Es especialmente útil para redistribuir la masa de probabilidad a n-gramas no observados (2 puntos)\n",
        "\n",
        "De la misma actividad en los modelos que exploramos anteriormente, utilizamos suavizado. ¿Qué sucede con los cálculos de perplejidad cuando no se aplica el suavizado?  A veces se utiliza el suavizado interpolado o de \"back-off\" en los modelos de lenguaje de n-gramas. Esta técnica calcula la probabilidad promedio ponderada de modelos con diferentes valores de `n`. Implementa esto. ¿Cómo afecta esto la perplejidad en el conjunto de pruebas retenido? ¿Qué pasa con la perplejidad sobre los datos de entrenamiento? (3 puntos)\n",
        "\n",
        "\n",
        "El término 'Naïve' en la clasificación por Naïve Bayes se refiere a la suposición de independencia e idéntica distribución. Extiende el clasificador Naïve Bayes utilizando el concepto de modelado de lenguaje bigrama. El nuevo modelo pierde el atributo 'Naïve'. ¿Puedes integrar características de bolsa de palabras en este modelo utilizando técnicas de suavizado? (3 puntos)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0559b314",
      "metadata": {
        "id": "0559b314"
      },
      "outputs": [],
      "source": [
        "## Tus respuestas\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c302556f",
      "metadata": {
        "id": "c302556f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}